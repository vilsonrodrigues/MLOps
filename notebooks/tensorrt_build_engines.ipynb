{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPrS8DEOEGluZVi9ED16tJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f4c52d9d5efa460ab3812a75be3a96e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55004c6e5ee74cf5855e31b41af78eae",
              "IPY_MODEL_280d870833444d3496d688cbb57b8af5",
              "IPY_MODEL_2ed18adbec524ed8b16fd1f4be71463e"
            ],
            "layout": "IPY_MODEL_18fdd1c0e65144c0b3fd7070df66f344"
          }
        },
        "55004c6e5ee74cf5855e31b41af78eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96cf81af1f4432b915425c700484367",
            "placeholder": "​",
            "style": "IPY_MODEL_2685615c065b481196ecb7f87be0deb8",
            "value": "model.safetensors: 100%"
          }
        },
        "280d870833444d3496d688cbb57b8af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50b7b46d13749cdae4fd069efa2919c",
            "max": 102469840,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688b1f70bccd43c6b5919757d3e2306f",
            "value": 102469840
          }
        },
        "2ed18adbec524ed8b16fd1f4be71463e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d3620e724c74e148b076ee3b53f98a3",
            "placeholder": "​",
            "style": "IPY_MODEL_20f79a5b138b4454ab16e8e8bc5fc6aa",
            "value": " 102M/102M [00:00&lt;00:00, 279MB/s]"
          }
        },
        "18fdd1c0e65144c0b3fd7070df66f344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a96cf81af1f4432b915425c700484367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2685615c065b481196ecb7f87be0deb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f50b7b46d13749cdae4fd069efa2919c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688b1f70bccd43c6b5919757d3e2306f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d3620e724c74e148b076ee3b53f98a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f79a5b138b4454ab16e8e8bc5fc6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vilsonrodrigues/MLOps/blob/main/notebooks/tensorrt_build_engines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NVIDIA TensorRT is an SDK for optimizing trained deep learning models to enable high-performance inference. TensorRT contains a deep learning inference optimizer for trained deep learning models, and a runtime for execution."
      ],
      "metadata": {
        "id": "QcilQN3HOcSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Definition**\n",
        "A representation of a model in TensorRT. A network definition is a graph of tensors and operators.\n",
        "\n",
        "**Builder**:\n",
        "TensorRT’s model optimizer. The builder takes as input a network definition, performs device-independent and device-specific optimizations, and creates an engine.\n",
        "\n",
        "**Engine**:\n",
        "A representation of a model that has been optimized by the TensorRT builder.\n",
        "\n",
        "**Logger**: Associated with the builder and engine to capture errors, warnings, and other information during the build and inference phases.\n",
        "\n",
        "**ONNX parser**: Takes a converted PyTorch trained model into the ONNX format as input and populates a network object in TensorRT.\n",
        "\n",
        "**Plan**:\n",
        "An optimized inference engine in a serialized format. To initialize the inference engine, the application will first deserialize the model from the plan file. A typical application will build an engine once, and then serialize it as a plan file for later use.\n",
        "\n",
        "**Runtime**:\n",
        "The component of TensorRT that performs inference on a TensorRT engine. The runtime API supports synchronous and asynchronous execution, profiling, and enumeration and querying of the bindings for an engine inputs and outputs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2wG7XvoTm4aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "a1n3wYrinWYa",
        "outputId": "35b45126-93e7-45d2-83c4-cbcd86bfd8b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Collecting triton==2.3.0 (from torch)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.20.5 torch-2.3.0 triton-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "triton"
                ]
              },
              "id": "521fa7aff6e44941af24902975ea06a0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt onnx"
      ],
      "metadata": {
        "id": "hZCoRwvXJ2tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Model to ONNX"
      ],
      "metadata": {
        "id": "7OZxs3eScubC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U timm>=0.9.0 torchvision"
      ],
      "metadata": {
        "id": "IjqTZgs9cjTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "\n",
        "model = timm.create_model(\"resnet50.a1_in1k\", pretrained=True)\n",
        "model = model.eval()\n",
        "\n",
        "# get model specific transforms (normalization, resize)\n",
        "data_config = timm.data.resolve_model_data_config(model)\n",
        "transforms = timm.data.create_transform(**data_config, is_training=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "f4c52d9d5efa460ab3812a75be3a96e9",
            "55004c6e5ee74cf5855e31b41af78eae",
            "280d870833444d3496d688cbb57b8af5",
            "2ed18adbec524ed8b16fd1f4be71463e",
            "18fdd1c0e65144c0b3fd7070df66f344",
            "a96cf81af1f4432b915425c700484367",
            "2685615c065b481196ecb7f87be0deb8",
            "f50b7b46d13749cdae4fd069efa2919c",
            "688b1f70bccd43c6b5919757d3e2306f",
            "2d3620e724c74e148b076ee3b53f98a3",
            "20f79a5b138b4454ab16e8e8bc5fc6aa"
          ]
        },
        "id": "TKpwGU0LcjKk",
        "outputId": "acd0afa6-96ee-4f30-bede-8a2b5f6cce7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4c52d9d5efa460ab3812a75be3a96e9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "channels = 3\n",
        "width = 224\n",
        "height = 224\n",
        "input_model = [channels, height, width]\n",
        "max_batch_size = 4"
      ],
      "metadata": {
        "id": "O14HTSSxj67X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape_input_model = [max_batch_size] + input_model"
      ],
      "metadata": {
        "id": "YgnreogLkMVT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape_input_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cM2vk_LkklN",
        "outputId": "c467d6c8-ef58-488b-88c1-484b1a10f417"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 3, 224, 224]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_input = torch.randn(shape_input_model)"
      ],
      "metadata": {
        "id": "D1jCL-8Zk9vg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAHo7Nhlk_hw",
        "outputId": "9c068e76-0e0c-4605-bd93-2f476ce79c58"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/onnx.html\n",
        "# Pytorch has two way to export model to ONNX\n",
        "# dynamo and script based\n",
        "# dynamo preserves the dynamic nature of the model instead\n",
        "# of using traditional static tracing techniques\n",
        "# But dynamo export in Pytorch 2.3.0 is still Beta\n",
        "# To apply TensorRT onnx parser the following exception is raised\n",
        "# the input ~input_name~ is duplicate\n",
        "\n",
        "if tensor_input.size(0) > 1:\n",
        "    dynamic = {\n",
        "        \"inputs\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
        "        \"outputs\": {0: \"batch\", 1: \"logits\"},\n",
        "    }\n",
        "else:\n",
        "    dynamic = None\n",
        "\n",
        "opset_version = 18\n",
        "f = \"model.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    tensor_input,\n",
        "    f,\n",
        "    verbose=True,\n",
        "    input_names=[\"inputs\"],\n",
        "    output_names=[\"outputs\"],\n",
        "    opset_version=opset_version,\n",
        "    do_constant_folding=True,  # torch>=1.12 require do_constant_folding=False\n",
        "    dynamic_axes=dynamic,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3wFvSWklKq9",
        "outputId": "f2787c02-33cd-4418-a547-6a8ecfc8d0e0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1547: OnnxExporterWarning: Exporting to ONNX opset version 18 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 17. To use a newer opset version, consider 'torch.onnx.dynamo_export()'. Note that dynamo_export() is in preview. Please report errors with dynamo_export() as Github issues to https://github.com/pytorch/pytorch/issues.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "model_onnx = onnx.load(f)\n",
        "onnx.checker.check_model(model_onnx)"
      ],
      "metadata": {
        "id": "ecnJt6ffhjv0"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_onnx.graph.input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnmN8L8g-xd-",
        "outputId": "672015f8-fcf0-4a52-dd7d-4c1d73f1e45f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"inputs\"\n",
            "type {\n",
            "  tensor_type {\n",
            "    elem_type: 1\n",
            "    shape {\n",
            "      dim {\n",
            "        dim_param: \"batch\"\n",
            "      }\n",
            "      dim {\n",
            "        dim_value: 3\n",
            "      }\n",
            "      dim {\n",
            "        dim_param: \"height\"\n",
            "      }\n",
            "      dim {\n",
            "        dim_param: \"width\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Engine"
      ],
      "metadata": {
        "id": "z5rOiY4bx1jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)"
      ],
      "metadata": {
        "id": "hnS6d0vF0dZQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = trt.Builder(TRT_LOGGER)\n",
        "config = builder.create_builder_config()"
      ],
      "metadata": {
        "id": "sTA9Zci02x-D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cache\n",
        "cache = config.create_timing_cache(b\"\")\n",
        "config.set_timing_cache(cache, ignore_mismatch=False)"
      ],
      "metadata": {
        "id": "IGzSjClptfRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#build_engine_python\n",
        "# Max Workspace define a memory limit to TensorRT layers\n",
        "# From documentation:\n",
        "# One important property is the maximum workspace size. Layer implementations often require a\n",
        "# temporary workspace, and this parameter limits the maximum size that any layer in the network\n",
        "# can use. If insufficient workspace is provided, it is possible that TensorRT will not be able\n",
        "# to find an implementation for a layer. By default, the workspace is set to the total global\n",
        "# memory size of the given device; restrict it when necessary, for example, when multiple engines\n",
        "# are to be built on a single device.\n",
        "\n",
        "# max_workspace = (1 << 30)\n",
        "# config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace)"
      ],
      "metadata": {
        "id": "qobrtPKk0c6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#version-compat\n",
        "# https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#explicit-implicit-batch\n",
        "# In implicit batch mode, every tensor has an implicit batch dimension and all other dimensions must have constant length.\n",
        "# In explicit batch mode, all dimensions are explicit and can be dynamic, that is their length can change at execution time.\n",
        "# Many new features, such as dynamic shapes and loops, are available only in this mode. It is also required by the ONNX parser.\n",
        "# In TensorRT 10 implicit batch is deprecated, explict batch is default is not possible disable\n",
        "flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "network = builder.create_network(flag)\n",
        "parser = trt.OnnxParser(network, TRT_LOGGER)"
      ],
      "metadata": {
        "id": "LrKLV5PA69yp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_onnx_model = \"./model.onnx\""
      ],
      "metadata": {
        "id": "aXVnFh7v9m7E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_onnx_model, \"rb\") as f:\n",
        "    if not parser.parse(f.read()):\n",
        "        print(f\"ERROR: Failed to parse the ONNX file {path_onnx_model}\")\n",
        "        for error in range(parser.num_errors):\n",
        "            print(parser.get_error(error))"
      ],
      "metadata": {
        "id": "g9fdBqTT9xWk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "outputs = [network.get_output(i) for i in range(network.num_outputs)]"
      ],
      "metadata": {
        "id": "RSdYX3UHToyB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYVv7mgzTtd3",
        "outputId": "a1b9dfdd-02c9-40ed-a5e7-e5f0a78a6cf1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorrt_bindings.tensorrt.ITensor at 0x7cabaef44f70>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuRYXmeuTxjE",
        "outputId": "82867887-9f03-4f79-8794-300bf03eb335"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorrt_bindings.tensorrt.ITensor at 0x7cabaef1b670>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input in inputs:\n",
        "    print(f\"Model {input.name} shape: {input.shape} {input.dtype}\")\n",
        "for output in outputs:\n",
        "    print(f\"Model {output.name} shape: {output.shape} {output.dtype}\")\n",
        "# https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work_dynamic_shapes\n",
        "# -1 indicates that dimension is runtime dimension\n",
        "# in build phase is not necessary specify to TensorRT\n",
        "# the real dimensions just in Runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz00B-WwUt3h",
        "outputId": "b4cdcc39-3691-41e3-e6ab-3e09c8069bab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model inputs shape: (-1, 3, -1, -1) DataType.FLOAT\n",
            "Model outputs shape: (-1, 1000) DataType.FLOAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTmdDEAMWSLd",
        "outputId": "315140e1-a2f6-4627-8c6f-c811f646c7f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape_input_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBsVsTgJXLKo",
        "outputId": "0d21afbe-021d-43a4-dce4-f5ef89fa37ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 3, 224, 224]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape_input_model[-3:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9pVaq_cPsl",
        "outputId": "061a9d8c-151f-4431-a8be-d69c7c85b294"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 224, 224]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if max_batch_size > 1:\n",
        "    # https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles\n",
        "    # To explict batch, set min, opt and max shape\n",
        "    # This help to TensorRT to search better optimizations\n",
        "    profile = builder.create_optimization_profile()\n",
        "    min_shape = [1] + shape_input_model[-3:]\n",
        "    opt_shape = [int(max_batch_size/2)] + shape_input_model[-3:]\n",
        "    max_shape = shape_input_model\n",
        "    for input in inputs:\n",
        "        profile.set_shape(input.name, min_shape, opt_shape, max_shape)\n",
        "    config.add_optimization_profile(profile)"
      ],
      "metadata": {
        "id": "GRnwONwZWhSl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.get_calibration_profile()"
      ],
      "metadata": {
        "id": "i_DSvh3DiYl-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if fast Half is avaliable\n",
        "builder.platform_has_fast_fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_efAOgRmd61Z",
        "outputId": "b0418cda-ee09-4699-8967-ac20e1520ea4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trt.BuilderFlag.BF16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgizkjPvkFEa",
        "outputId": "f810cef4-f040-419e-ef3c-ccd3cabcf7b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BuilderFlag.BF16: 17>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#reduced-precision\n",
        "# https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix\n",
        "# Reduce precision. Has three options: FP16, INT8 and TF32 (Tensor Cores)\n",
        "# Note that TensorRT will still choose a higher-precision kernel if it\n",
        "# results in overall lower runtime, or if no low-precision implementation exists.\n",
        "half = True\n",
        "int8 = False\n",
        "if half:\n",
        "    config.set_flag(trt.BuilderFlag.FP16)\n",
        "elif int8:\n",
        "    config.set_flag(trt.BuilderFlag.INT8)"
      ],
      "metadata": {
        "id": "o6XzSWJ30cYz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#weightless-build\n",
        "# https://github.com/NVIDIA/TensorRT/tree/main/samples/python/sample_weight_stripping\n",
        "# Help to create and optimize an engine without unnecessary weights\n",
        "# On inference load engine and refit with onnx weights\n",
        "# It`s more fast and no duplicate weights\n",
        "strip_weights = False\n",
        "if strip_weights:\n",
        "    config.set_flag(trt.BuilderFlag.STRIP_PLAN)\n",
        "# To remove strip plan from config\n",
        "# config.flags &= ~(1 << int(trt.BuilderFlag.STRIP_PLAN))"
      ],
      "metadata": {
        "id": "vYd4urgDtDrA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build engine\n",
        "engine_bytes = builder.build_serialized_network(network, config)"
      ],
      "metadata": {
        "id": "C5J2r0ZY0b73"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine_path = \"./model.engine\"\n",
        "with open(engine_path, \"wb\") as f:\n",
        "    f.write(engine_bytes)"
      ],
      "metadata": {
        "id": "QO4w9SsG0be3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Engine"
      ],
      "metadata": {
        "id": "HRpOOnoAnt7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab bug...\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "0nKlGFjH_gHg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cuda-python>=12.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9Ptv_aRBFyn",
        "outputId": "1b3773fe-1a94-43e6-c0d3-5c2c94e71169"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cuda-python\n",
            "  Downloading cuda_python-12.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cuda-python\n",
            "Successfully installed cuda-python-12.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stripped_engine_and_refit(\n",
        "    engine_path: str,\n",
        "    onnx_model_path: str,\n",
        ") -> trt.ICudaEngine:\n",
        "    runtime = trt.Runtime(TRT_LOGGER)\n",
        "    with open(engine_path, \"rb\") as engine_file:\n",
        "        engine = runtime.deserialize_cuda_engine(engine_file.read())\n",
        "        refitter = trt.Refitter(engine, TRT_LOGGER)\n",
        "        parser_refitter = trt.OnnxParserRefitter(refitter, TRT_LOGGER)\n",
        "        assert parser_refitter.refit_from_file(onnx_model_path)\n",
        "        assert refitter.refit_cuda_engine()\n",
        "        return engine\n",
        "\n",
        "def load_normal_engine(engine_path: str) -> trt.ICudaEngine:\n",
        "    runtime = trt.Runtime(TRT_LOGGER)\n",
        "    with open(engine_path, \"rb\") as plan:\n",
        "        engine = runtime.deserialize_cuda_engine(plan.read())\n",
        "        return engine"
      ],
      "metadata": {
        "id": "yMYhbiIN4X8k"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if strip_weights:\n",
        "    engine = load_stripped_engine_and_refit(engine_path, path_onnx_model)\n",
        "else:\n",
        "    engine = load_normal_engine(engine_path)"
      ],
      "metadata": {
        "id": "7cEmDc7032xF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7eftiNb70rv",
        "outputId": "5a846b36-6171-4f00-cd10-e9762d6d925a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorrt_bindings.tensorrt.ICudaEngine at 0x7caa3f5d32f0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}